<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Evaluating LLM consistency with exponentially less data | Sundai Research</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Evaluating LLM consistency with exponentially less data" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Two weeks ago, we built an LLM consistency leaderboard to shed light on the consistency of LLMs (reproduced here for your convenience). Keep in mind that we are ranking models by their consistency: if a model is consistently bad, then it’ll rank highly on the consistency leaderboard." />
<meta property="og:description" content="Two weeks ago, we built an LLM consistency leaderboard to shed light on the consistency of LLMs (reproduced here for your convenience). Keep in mind that we are ranking models by their consistency: if a model is consistently bad, then it’ll rank highly on the consistency leaderboard." />
<link rel="canonical" href="/projects/2025-04-06-efficient-consistency-evaluation" />
<meta property="og:url" content="/projects/2025-04-06-efficient-consistency-evaluation" />
<meta property="og:site_name" content="Sundai Research" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-04-06T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Evaluating LLM consistency with exponentially less data" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-04-06T00:00:00-07:00","datePublished":"2025-04-06T00:00:00-07:00","description":"Two weeks ago, we built an LLM consistency leaderboard to shed light on the consistency of LLMs (reproduced here for your convenience). Keep in mind that we are ranking models by their consistency: if a model is consistently bad, then it’ll rank highly on the consistency leaderboard.","headline":"Evaluating LLM consistency with exponentially less data","mainEntityOfPage":{"@type":"WebPage","@id":"/projects/2025-04-06-efficient-consistency-evaluation"},"url":"/projects/2025-04-06-efficient-consistency-evaluation"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/css/style.css">
  <!--[if lt IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
  <![endif]-->
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
  <script src="https://kit.fontawesome.com/2547d58cc9.js" crossorigin="anonymous"></script>
  
  <script src="/assets/js/mathjax-config.js" type="text/javascript"></script>
  
  <!-- <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon/apple-touch-icon.png"> -->
</head>
  <body>
    <div id="container">

      <header>
  <a href="/">
    <div class="logo-container">
      <div class="logo-line" id="sundai"><h1>Sundai</h1></div>
      <div class="logo-line" id="research"><h1>Research</h1></div>
    </div>  
  </a>
  <nav>
    <ul><li>
        <a href="/projects/">Projects</a>
      </li></ul>
  </nav>
</header>

      <section>

      <h1>Evaluating LLM consistency with exponentially less data</h1>

<p>Two weeks ago, we built an <a href="/projects/2025-03-23-consistency-leaderboard">LLM consistency leaderboard</a> to shed light on the <em>consistency</em> of LLMs (reproduced here for your convenience). Keep in mind that we are ranking models by their consistency: if a model is consistently bad, then it’ll rank highly on the consistency leaderboard.</p>

<script src="/assets/js/sorttable.js"></script>

<table class="sortable">
  <caption>
    <p><strong>LLM consistency leaderboard:</strong> click on the column headers to sort the rows by the entries in the respective columns</p>
  </caption>
  <thead>
    <tr>
      <th style="text-align: left;">LLM</th>
      <th style="text-align: right;">total</th>
      <th style="text-align: right;">enumerator</th>
      <th style="text-align: right;">separator</th>
      <th style="text-align: right;">choices order</th>
      <th style="text-align: right;">phrasing</th>
      <!-- <th style="text-align: right;">residual</th> -->
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left;"><code>Mistral-7B-Instruct-v0.3</code></td>
      <td style="text-align: right;">1377.250181</td>
      <td style="text-align: right;">121.579405</td>
      <td style="text-align: right;">25.363689</td>
      <td style="text-align: right;">85.748062</td>
      <td style="text-align: right;">319.026200</td>
      <!-- <td style="text-align: right;">825.532825</td> -->
    </tr>
    <tr>
      <td style="text-align: left;"><code>Llama-3.2-1B-Instruct</code></td>
      <td style="text-align: right;">1713.863161</td>
      <td style="text-align: right;">167.214951</td>
      <td style="text-align: right;">27.546341</td>
      <td style="text-align: right;">281.520735</td>
      <td style="text-align: right;">6.358699</td>
      <!-- <td style="text-align: right;">1231.222435</td> -->
    </tr>
    <tr>
      <td style="text-align: left;"><code>OLMoE-1B-7B-0924-Instruct</code></td>
      <td style="text-align: right;">2082.808656</td>
      <td style="text-align: right;">130.161762</td>
      <td style="text-align: right;">20.784928</td>
      <td style="text-align: right;">12.895170</td>
      <td style="text-align: right;">231.941715</td>
      <!-- <td style="text-align: right;">1687.025081</td> -->
    </tr>
    <tr>
      <td style="text-align: left;"><code>Llama-3.2-3B-Instruct</code></td>
      <td style="text-align: right;">2305.981318</td>
      <td style="text-align: right;">470.136490</td>
      <td style="text-align: right;">18.672984</td>
      <td style="text-align: right;">205.075207</td>
      <td style="text-align: right;">208.307431</td>
      <!-- <td style="text-align: right;">1403.789206</td> -->
    </tr>
    <tr>
      <td style="text-align: left;"><code>Meta-Llama-3-8B-Instruct</code></td>
      <td style="text-align: right;">2368.475849</td>
      <td style="text-align: right;">279.759745</td>
      <td style="text-align: right;">3.722368</td>
      <td style="text-align: right;">190.237480</td>
      <td style="text-align: right;">566.256863</td>
      <!-- <td style="text-align: right;">1328.499393</td> -->
    </tr>
  </tbody>
</table>

<p>There are only 5 models on the leaderboard because there are only 5 LLMs in (the lite version of) the <a href="https://slab-nlp.github.io/DOVE/">Dataset Of Variation Evaluation (DOVE)</a> that the leaderboard relies on. Despite “only” measuring the effects of 5 prompt perturbations on 5 LLMs on 392 benchmarks, DOVE Lite takes up 100GB on disk, and its creators estimate its creation cost $1.25k-ish on AWS. The full version of DOVE is 20x larger: it takes up 2TB on disk and costs $25k-ish to create. DOVE (Lite) is so large because it records the benchmark results of the models with <em>all possible combinations of the prompt perturbations</em> (so called full factorial experiments). As we shall see, to develop the LLM consistency leaderboard, such full factorial experiments are overkill.</p>

<p>Let \(X\) and \(y\) be the design matrix and outcome (accuracy) vector for an LLM on all \(B\) benchmarks:</p>

\[X = \left[\begin{array}{c|c|c|c}1_ne_1^\top &amp; X_{1,1} &amp; \dots &amp; X_{K,1} \\
\vdots &amp; \vdots &amp; \dots &amp; \vdots \\ 1_ne_B^\top &amp; X_{1,B} &amp; \dots &amp; X_{K,B}\end{array}\right]\quad y = \begin{bmatrix}y_1 \\ \vdots \\ y_B\end{bmatrix},\]

<p>where \(e_b\) is the \(b\)-th column of \(I_B\), \(X_{b,k}\) is the dummy variable matrix associated with the \(k\)-th prompt perturbation in the \(b\)-th benchmark, and \(y_b\) is the outcome vector from the \(b\)-th benchmark. The main idea in the LLM consistency leaderboard is a <strong>decomposition of variance (DoV):</strong></p>

\[\textstyle
\|y - X_0X_0^\top y\|_2^2 = \sum_{k=1}^K\|X_kX_k^\top y\|_2^2 + \|(I_n - XX^\top)y\|_2^2,
\tag{DoV}\]

<p>where \(X_0\) is the leftmost block of \(X\) that indicates the benchmark and the rest of the \(X_k\)’s consist of \(X_{k,1},\dots,X_{k,B}\) stacked vertically. The terms in (DoV) appear in the rows of the consistency leaderboard:</p>

<ul>
  <li>\(\|y - X_0X_0^\top y\|_2^2\) appears in the “total” column: it is the sum of variances in the LLM’s accuracy;</li>
  <li>\(\|X_kX_k^\top y\|_2^2\) appear in the rest of the columns: they are the variance (in the LLM’s accuracy) due to the individual prompt perturbations.</li>
</ul>

<p>To reduce the computational (and monetary) cost of assessing the consistency of an LLM, we consider <em>subsampling</em> the rows of \(X\) and entries of \(y\). This allows us to get away with running a fraction of the experiments in the full factorial design. At a high-level, we</p>

<ol>
  <li>subsample the rows of \(X\in\reals^{N\times d}\) and the entries of \(y\in\reals^N\) to obtain \(X'\in\reals^{n\times d}\) and \(y'\in\reals^n\) (\(n\ll N\)); this reduces the size of the subsampled design from \(N\) to \(n\);</li>
  <li>estimate the \(\|X_kX_k^\top y\|_2^2\) terms in (DoV) with \(\frac{N}{n}\|(\Omega X)^\top\Omega y\|_2\), where \(\Omega\in\reals^{n\times N}\) is the subsampling matrix;</li>
  <li>estimate the total variance term in (DoV) with \(\|y^\top S^\top S y\|_2^2 - \|X_k^\top S^\top S y\|_2^2.\)</li>
</ol>

<p>In the rest of the post, let \(S\triangleq(\frac{N}{n})^{\frac12}\Omega\). The subsampled design works because of the (non-trivial) fact that \(S\) is an approximate isometry (in two ways):</p>

<ol>
  <li>\(\|X^\top S^\top Sy - X^\top y\|_2\le \eps\|y\|_2\).</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$$</td>
          <td>y^\top S^\top Sy - y^\top y</td>
          <td>\le \eps$$,</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<p>Such an \(S\) is called an <strong>\(\eps\)-subspace embedding</strong> in the randomized numerical linear algebra (randNLA) literature because it preserves the geometry of \(\cR(X)\). To see why we expect \(\frac{N}{n}\|X^\top S^\top S y\|_2\) to be a good estimate of \(\|X_kX_k^\top y\|_2^2 = \|X_k^\top y\|_2^2\), first convince yourself that the two preceding approximate isometry conditions imply their counterparts with \(X_k\) instead of \(X\).</p>


<p><small>Hacked together by UMich FATML on April 06, 2025
</small></p>

      </section>

      <footer>
  <p><small>Hosted on <a href="http://github.com">GitHub</a> | Powered by <a href="http://jekyllrb.com/">Jekyll</a></small></p>
</footer>

    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
    <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    
    
  </body>
</html>