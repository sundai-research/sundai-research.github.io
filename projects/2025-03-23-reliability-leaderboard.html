<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Reliability leaderboard | Sundai Research</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Reliability leaderboard" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We built a reliability leaderboard to shed light on the reliability of LLMs. By reliability, we mean the model’s performance should be insensitive to prompt perturbations that are semantically equivalent to the original prompt. In other words, a reliable LLM should perform the same regardless of how exactly it is prompted (as long as the crux of the prompt remains the same). For multiple choice question-answering (QA) tasks, some examples of prompt perturbations that should not affect the model’s performance include:" />
<meta property="og:description" content="We built a reliability leaderboard to shed light on the reliability of LLMs. By reliability, we mean the model’s performance should be insensitive to prompt perturbations that are semantically equivalent to the original prompt. In other words, a reliable LLM should perform the same regardless of how exactly it is prompted (as long as the crux of the prompt remains the same). For multiple choice question-answering (QA) tasks, some examples of prompt perturbations that should not affect the model’s performance include:" />
<link rel="canonical" href="/projects/2025-03-23-reliability-leaderboard" />
<meta property="og:url" content="/projects/2025-03-23-reliability-leaderboard" />
<meta property="og:site_name" content="Sundai Research" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-23T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Reliability leaderboard" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-23T00:00:00-04:00","datePublished":"2025-03-23T00:00:00-04:00","description":"We built a reliability leaderboard to shed light on the reliability of LLMs. By reliability, we mean the model’s performance should be insensitive to prompt perturbations that are semantically equivalent to the original prompt. In other words, a reliable LLM should perform the same regardless of how exactly it is prompted (as long as the crux of the prompt remains the same). For multiple choice question-answering (QA) tasks, some examples of prompt perturbations that should not affect the model’s performance include:","headline":"Reliability leaderboard","mainEntityOfPage":{"@type":"WebPage","@id":"/projects/2025-03-23-reliability-leaderboard"},"url":"/projects/2025-03-23-reliability-leaderboard"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/css/style.css">
  <!--[if lt IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
  <![endif]-->
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
  <script src="https://kit.fontawesome.com/ef661a1083.js" crossorigin="anonymous"></script>
  
  <script src="/assets/js/mathjax-config.js" type="text/javascript"></script>
  
  <!-- <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon/apple-touch-icon.png"> -->
</head>
  <body>
    <div id="container">

      <header>
  <a href="/">
    <div class="logo-container">
      <div class="logo-line" id="sundai"><h1>Sundai</h1></div>
      <div class="logo-line" id="research"><h1>Research</h1></div>
    </div>  
  </a>
  <nav>
    <ul><li>
        <a href="/projects/">Projects</a>
      </li></ul>
  </nav>
</header>

      <section>

      <h1>Reliability leaderboard</h1>

<p>We built a reliability leaderboard to shed light on the <em>reliability</em> of LLMs. By reliability, we mean the model’s performance should be insensitive to  prompt perturbations that are <em>semantically equivalent</em> to the original prompt. In other words, a reliable LLM should perform the same regardless of how exactly it is prompted (as long as the crux of the prompt remains the same). For multiple choice question-answering (QA) tasks, some examples of <em>prompt perturbations</em> that should not affect the model’s performance include:</p>

<ol>
  <li>changing the way the list of answer choices is numbered (eg from arabic to roman numerals),</li>
  <li>changing the way the question is phrased (eg from “What is the capital of France?” to “France’s capital is”),</li>
  <li>changing the order of the answer choices.</li>
</ol>

<p>The main statistical concept that we rely on to assess LLM reliability is the <em>decomposition of variance</em>, which allows us to attribute the variance in the LLMs performance to the prompt perturbations.</p>

<!-- __Notation/terminology:__ A prompt variation is a (distinct) combination of prompt perturbations, and we encode prompt variations with vectors of categorical variables $$x\in\reals^d$$, where $$d$$ is the total number of prompt perturbations. For example, if there are 2 prompt perturbations (eg the first 2 listed above) and 3, 2 possible values for the 2 prompt perturbations respectively, then there are $$x\in\reals^2$$ has $$3\times 2 = 6$$ combinations of the prompt perturbations. Let $$y\in\reals$$ be a measure (eg accuracy) of an LLM performance on a benchmark.  -->

<p>We use the <a href="https://slab-nlp.github.io/DOVE/">Dataset Of Variation Evaluation (DOVE)</a> to assess the reliability of LLMs. This dataset records the results from (mixed-level) <em>full factorial</em> experiments, where the factors are the prompt perturbations, the levels are the possible values of the prompt perturbations, and outcome is an LLM’s (eg <code class="language-plaintext highlighter-rouge">Meta-Llama-3-8B-Instruct</code>’s) accuracy on a benchmark (eg HellaSwag). The dataset includes the results of \(392\times 5 = 1960\) experiments, where each experiment corresponds to a distinct benchmark-LLM pair.</p>

<p>For now, we focus on a single experiment in the DOVE dataset. Let \(X\in\{0,1\}^{n\times d}\) be the design matrix and \(y\in\reals^n\) be the outcome vector (so the rows of \(X\) encodes a combination of prompt perturbations and the corresponding entries in \(y\) are the LLM’s accuracy on the benchmark). The design matrix has the form</p>

\[X = \left[\begin{array}{c|c|c|c}1_n &amp; X_1 &amp; \dots &amp; X_K\end{array}\right],\]

<p>where \(1_n\) is a vector of ones (encoding the intercept term) and \(X_k\) is a 0/1 matrix encoding the \(L_k-1\) dummy variables indicating the levels of the \(k\)-th prompt perturbation. In a full-factorial experiment, the design matrix has orthogonal columns, so it is possible to decompose the variance of \(y\) as</p>

\[\textstyle
\|y - 1_n\bar{y}\|_2^2 = \sum_{k=1}^K\|X_kX_k^\top y\|_2^2 + \|(I_n - XX^\top)y\|_2^2,
\tag{DoV}\]

<p>where \(\bar{y}\triangleq\frac1n1_n^\top y\) is the average outcome and \(X_k\) is the submatrix of \(X\) associated with the \(k\)-th prompt perturbation. We interpret the terms in (DoV):</p>

<ul>
  <li>\(\|y - 1_n\bar{y}\|_2^2\) is the total variance in the LLM’s accuracy due to (all) prompt perturbations; it measures the height of the violin plots in figure 3 in <a href="https://arxiv.org/abs/2503.01622">the DOVE paper</a>;</li>
  <li>\(\|X_kX_k^\top y\|_2^2\) is the variance (in the LLM’s accuracy) due to the \(k\)-th prompt perturbation;</li>
  <li>\(\|(I_n - XX^\top)y\|_2^2\) is the residual variance that cannot be attributed to any prompt perturbation.</li>
</ul>

<p>For example, the total variance of <code class="language-plaintext highlighter-rouge">Meta-Llama-3-8B-Instruct</code>’s accuracy on <code class="language-plaintext highlighter-rouge">mmlu.high_school_macroeconomics</code> is 43.30, and it can be attributed to the prompt perturbations as follows:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                     sum_sq      df
C(enumerator)      4.025135     5.0
C(separator)       0.912668     6.0
C(choices_order)   7.524167     6.0
C(phrasing)       16.990095    12.0
Residual          12.846194  3792.0
</code></pre></div></div>
<p>We see that</p>

<ol>
  <li>the largest fraction of the variance is due to rephrasing the prompt;</li>
  <li>changing the separator between answer choices seems to have almost no effect on the LLM’s accuracy (it accounts for 2% of the variance in the LLM’s accuracy);</li>
  <li>there is a significant fraction of residual variance that cannot be attributed to the main effects; we must consider interactions among prompt perturbations to explain this residual variance.</li>
</ol>

<p>The preceding variance decomposition decomposes the variance in an LLM’s accuracy on a benchmark. To assess the overall reliability of an LLM, we wish to aggregate the variance decompositions across multiple benchmarks. The relies on a generalization of (DoV) to multiple experiment. We start by stacking the design matrices and outcome vectors from multiple experiments to form a single very tall design matrix and single very long outcome vector:</p>

\[X = \left[\begin{array}{c|c|c|c}1_ne_1^\top &amp; X_{1,1} &amp; \dots &amp; X_{K,1} \\
\vdots &amp; \vdots &amp; \dots &amp; \vdots \\ 1_ne_B^\top &amp; X_{1,B} &amp; \dots &amp; X_{K,B}\end{array}\right]\quad y = \begin{bmatrix}y_1 \\ \vdots \\ y_B\end{bmatrix},\]

<p>where \(e_b\) is the \(b\)-th column of \(I_B\), \(X_{b,k}\) is the 0/1 matrix associated with the \(k\)-th prompt perturbation in the \(b\)-th experiment, and \(y_b\) is the outcome vector from the \(b\)-th experiment. We leave as an exercise to the reader to check that</p>

\[\textstyle
\|y - EE^\top y\|_2^2 = \sum_{k=1}^K\|X_kX_k^\top y\|_2^2 + \|(I_n - XX^\top)y\|_2^2,
\tag{DoV2}\]

<p>where \(E\) is the left-most block of \(X\). We emphasize that the matrices \(X_k\) and \(X\) and vector \(y\) in (DoV2) are the stacked versions of the eponymous matrices and vector in (DoV). The terms in (DoV2) have similar interpretations as their counterparts in (DoV):</p>

<ul>
  <li>\(\|y - EE^\top y\|_2^2\) is the sum of variances in the LLM’s accuracy due to (all) prompt perturbations <em>across all benchmarks</em>;</li>
  <li>\(\|X_kX_k^\top y\|_2^2\) is the variance (in the LLM’s accuracy) due to the \(k\)-th prompt perturbation <em>across all benchmarks</em>;</li>
  <li>\(\|(I_n - XX^\top)y\|_2^2\) is the residual variance that cannot be attributed to any prompt perturbation.</li>
</ul>

<p>We evaluate (DoV2) for <code class="language-plaintext highlighter-rouge">Meta-Llama-3-8B-Instruct</code>, <code class="language-plaintext highlighter-rouge">Mistral-7B-Instruct-v0.3</code>,and <code class="language-plaintext highlighter-rouge">OLMoE-1B-7B-0924-Instruct</code> to obtain the following overall variance decompositions:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>                  Meta-Llama-3-8B-Instruct  Mistral-7B-Instruct-v0.3  OLMoE-1B-7B-0924-Instruct
                          sum_sq        df          sum_sq        df           sum_sq        df
C(benchmark)         5096.380943      57.0     3157.264186      57.0      2022.705493      50.0
C(enumerator)         279.759745       5.0      121.579405       5.0       130.161762       5.0
C(separator)            3.722368       6.0       25.363689       6.0        20.784928       6.0
C(choices_order)      190.237480       6.0       85.748062       6.0        12.895170       6.0
C(phrasing)           566.256863      18.0      319.026200      18.0       231.941715      18.0
Residual             1328.499393  221583.0      825.532825  221583.0      1687.025081  194836.0
</code></pre></div></div>
<p>We see that</p>

<ol>
  <li>the largest fraction of the variance in the LLMs’ accuracies is due to rephrasing the prompt, but changing the enumerator and answer choices order also account for significant fractions of the variance</li>
  <li>changing the separator seems to have almost no effect on the LLMs’ accuracies;</li>
  <li>there is a significant fraction of residual variance that cannot be attributed to the main effects; we must consider interactions among prompt perturbations to explain this residual variance.</li>
</ol>


<p><small>Hacked together by Seamus Somerstep and Yuekai Sun on March 23, 2025
</small></p>

      </section>

      <footer>
  <p><small>Hosted on <a href="http://github.com">GitHub</a> | Powered by <a href="http://jekyllrb.com/">Jekyll</a></small></p>
</footer>

    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
    <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    
    
  </body>
</html>