<!DOCTYPE html>
<html lang="en-US">
  <head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>LLM consistency leaderboard | Sundai Research</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="LLM consistency leaderboard" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We built an LLM consistency leaderboard to shed light on the consistency of LLMs. By consistency, we mean the model’s performance should be insensitive to prompt perturbations that are semantically equivalent to the original prompt. In other words, a consistent LLM should perform the same regardless of how exactly it is prompted (as long as the crux of the prompt remains the same). For multiple choice question-answering (QA) tasks, some examples of prompt perturbations that should not affect the model’s performance include:" />
<meta property="og:description" content="We built an LLM consistency leaderboard to shed light on the consistency of LLMs. By consistency, we mean the model’s performance should be insensitive to prompt perturbations that are semantically equivalent to the original prompt. In other words, a consistent LLM should perform the same regardless of how exactly it is prompted (as long as the crux of the prompt remains the same). For multiple choice question-answering (QA) tasks, some examples of prompt perturbations that should not affect the model’s performance include:" />
<link rel="canonical" href="/projects/2025-03-23-consistency-leaderboard" />
<meta property="og:url" content="/projects/2025-03-23-consistency-leaderboard" />
<meta property="og:site_name" content="Sundai Research" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-23T00:00:00-07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="LLM consistency leaderboard" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-23T00:00:00-07:00","datePublished":"2025-03-23T00:00:00-07:00","description":"We built an LLM consistency leaderboard to shed light on the consistency of LLMs. By consistency, we mean the model’s performance should be insensitive to prompt perturbations that are semantically equivalent to the original prompt. In other words, a consistent LLM should perform the same regardless of how exactly it is prompted (as long as the crux of the prompt remains the same). For multiple choice question-answering (QA) tasks, some examples of prompt perturbations that should not affect the model’s performance include:","headline":"LLM consistency leaderboard","mainEntityOfPage":{"@type":"WebPage","@id":"/projects/2025-03-23-consistency-leaderboard"},"url":"/projects/2025-03-23-consistency-leaderboard"}</script>
<!-- End Jekyll SEO tag -->

  <link rel="stylesheet" href="/assets/css/style.css">
  <!--[if lt IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
  <![endif]-->
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet">
  <script src="https://kit.fontawesome.com/2547d58cc9.js" crossorigin="anonymous"></script>
  
  <script src="/assets/js/mathjax-config.js" type="text/javascript"></script>
  
  <!-- <link rel="shortcut icon" type="image/x-icon" href="/assets/favicon/favicon.ico">
  <link rel="apple-touch-icon-precomposed" href="/assets/favicon/apple-touch-icon.png"> -->
</head>
  <body>
    <div id="container">

      <header>
  <a href="/">
    <div class="logo-container">
      <div class="logo-line" id="sundai"><h1>Sundai</h1></div>
      <div class="logo-line" id="research"><h1>Research</h1></div>
    </div>  
  </a>
  <nav>
    <ul><li>
        <a href="/projects/">Projects</a>
      </li></ul>
  </nav>
</header>

      <section>

      <h1>LLM consistency leaderboard</h1>

<p>We built an LLM consistency leaderboard to shed light on the <em>consistency</em> of LLMs. By consistency, we mean the model’s performance should be insensitive to  prompt perturbations that are <em>semantically equivalent</em> to the original prompt. In other words, a consistent LLM should perform the same regardless of how exactly it is prompted (as long as the crux of the prompt remains the same). For multiple choice question-answering (QA) tasks, some examples of <em>prompt perturbations</em> that should not affect the model’s performance include:</p>

<ol>
  <li>changing the way the list of answer choices is numbered (eg from arabic to roman numerals),</li>
  <li>changing the way the question is phrased (eg from “What is the capital of France?” to “France’s capital is”),</li>
  <li>changing the order of the answer choices.</li>
</ol>

<p>The main statistical concept that we rely on to assess LLM consistency is the <em>decomposition of variance</em>, which allows us to attribute the variance in the LLMs performance to the prompt perturbations.</p>

<p>We use the lite version of <a href="https://slab-nlp.github.io/DOVE/">Dataset Of Variation Evaluation (DOVE)</a> to assess the consistency of LLMs. This dataset records the results from (mixed-level) <em>full factorial</em> experiments, where the factors are the prompt perturbations, the levels are the possible values of the prompt perturbations, and outcome is an LLM’s (eg <code class="language-plaintext highlighter-rouge">Meta-Llama-3-8B-Instruct</code>’s) accuracy on a benchmark (eg HellaSwag). The dataset includes the results of \(392\times 5 = 1960\) experiments, where each experiment corresponds to a distinct benchmark-LLM pair.</p>

<p>For now, we focus on a single experiment in the DOVE dataset. Let \(X\in\{0,1\}^{n\times d}\) be the design matrix and \(y\in\reals^n\) be the outcome vector (so the rows of \(X\) encodes a combination of prompt perturbations and the corresponding entries in \(y\) are the LLM’s accuracy on the benchmark). The design matrix has the form</p>

\[X = \left[\begin{array}{c|c|c|c}1_n &amp; X_1 &amp; \dots &amp; X_K\end{array}\right],\]

<p>where \(1_n\) is a vector of ones (encoding the intercept term) and \(X_k\) is a 0/1 matrix encoding the \(L_k-1\) dummy variables indicating the levels of the \(k\)-th prompt perturbation. In a full-factorial experiment, the design matrix is orthogonal (ie it has orthogonal columns), so it is possible to decompose the variance of \(y\) as</p>

\[\textstyle
\|y - 1_n\bar{y}\|_2^2 = \sum_{k=1}^K\|X_kX_k^\top y\|_2^2 + \|(I_n - XX^\top)y\|_2^2,
\tag{DoV}\]

<p>where \(\bar{y}\triangleq\frac1n1_n^\top y\) is the average outcome and \(X_k\) is the submatrix of \(X\) associated with the \(k\)-th prompt perturbation. We interpret the terms in (DoV):</p>

<ul>
  <li>\(\|y - 1_n\bar{y}\|_2^2\) is the total variance in the LLM’s accuracy due to (all) prompt perturbations; it measures the height of the violin plots in figure 3 in <a href="https://arxiv.org/abs/2503.01622">the DOVE paper</a>;</li>
  <li>\(\|X_kX_k^\top y\|_2^2\) is the variance (in the LLM’s accuracy) due to the \(k\)-th prompt perturbation;</li>
  <li>\(\|(I_n - XX^\top)y\|_2^2\) is the residual variance that cannot be attributed to any prompt perturbation.</li>
</ul>

<p>For example, we decompose the variance of <code class="language-plaintext highlighter-rouge">Meta-Llama-3-8B-Instruct</code>’s accuracy on 3 subsets of MMLU with (DoV) to obtain the following variance decomposition:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">prompt perturbation</th>
      <th style="text-align: right"><code class="language-plaintext highlighter-rouge">hs_macroeconomics</code></th>
      <th style="text-align: right"><code class="language-plaintext highlighter-rouge">misc</code></th>
      <th style="text-align: right"><code class="language-plaintext highlighter-rouge">public_relations</code></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">enumerator</td>
      <td style="text-align: right">4.025135</td>
      <td style="text-align: right">5.637558</td>
      <td style="text-align: right">4.119202</td>
    </tr>
    <tr>
      <td style="text-align: left">separator</td>
      <td style="text-align: right">0.912668</td>
      <td style="text-align: right">0.539501</td>
      <td style="text-align: right">0.469672</td>
    </tr>
    <tr>
      <td style="text-align: left">choices order</td>
      <td style="text-align: right">7.524167</td>
      <td style="text-align: right">3.479708</td>
      <td style="text-align: right">8.747673</td>
    </tr>
    <tr>
      <td style="text-align: left">phrasing</td>
      <td style="text-align: right">16.990095</td>
      <td style="text-align: right">14.544431</td>
      <td style="text-align: right">2.711865</td>
    </tr>
    <tr>
      <td style="text-align: left">residual</td>
      <td style="text-align: right">12.846194</td>
      <td style="text-align: right">13.013123</td>
      <td style="text-align: right">12.101369</td>
    </tr>
  </tbody>
</table>

<p>We see that</p>

<ol>
  <li>the largest fraction of the variance is due to rephrasing the prompt;</li>
  <li>changing the separator between answer choices seems to have almost no effect on the LLM’s accuracy (it accounts for 2% of the variance in the LLM’s accuracy);</li>
  <li>there is a significant fraction of residual variance that cannot be attributed to the main effects; we must consider interactions among prompt perturbations to explain this residual variance.</li>
</ol>

<p>The preceding variance decomposition decomposes the variance in an LLM’s accuracy on a benchmark. To assess the overall consistency of an LLM, we wish to aggregate the variance decompositions across multiple benchmarks. The relies on a generalization of (DoV) to multiple benchmarks. We start by stacking the design matrices and outcome vectors from multiple benchmarks to form a single very tall design matrix and single very long outcome vector:</p>

\[X = \left[\begin{array}{c|c|c|c}1_ne_1^\top &amp; X_{1,1} &amp; \dots &amp; X_{K,1} \\
\vdots &amp; \vdots &amp; \dots &amp; \vdots \\ 1_ne_B^\top &amp; X_{1,B} &amp; \dots &amp; X_{K,B}\end{array}\right]\quad y = \begin{bmatrix}y_1 \\ \vdots \\ y_B\end{bmatrix},\]

<p>where \(e_b\) is the \(b\)-th column of \(I_B\), \(X_{b,k}\) is the dummy variable matrix associated with the \(k\)-th prompt perturbation in the \(b\)-th benchmark, and \(y_b\) is the outcome vector from the \(b\)-th benchmark. We leave as an exercise to the reader to check that</p>

\[\textstyle
\|y - X_0X_0^\top y\|_2^2 = \sum_{k=1}^K\|X_kX_k^\top y\|_2^2 + \|(I_n - XX^\top)y\|_2^2,
\tag{DoV2}\]

<p>where \(X_0\) is the leftmost block of \(X\) that indicates the benchmark. We emphasize that the matrices \(X_k\) and \(X\) and vector \(y\) in (DoV2) are the stacked versions of the eponymous matrices and vector in (DoV). The terms in (DoV2) have similar interpretations as their counterparts in (DoV):</p>

<ul>
  <li>\(\|y - X_0X_0^\top y\|_2^2\) is the sum of variances across all benchmarks in the LLM’s accuracy;</li>
  <li>\(\|X_kX_k^\top y\|_2^2\) is the variance (in the LLM’s accuracy) due to the \(k\)-th prompt perturbation (across all benchmarks);</li>
  <li>\(\|(I_n - XX^\top)y\|_2^2\) is the residual variance that cannot be attributed to any prompt perturbation.</li>
</ul>

<p>For example, we decompose the total variance of <code class="language-plaintext highlighter-rouge">Meta-Llama-3-8B-Instruct</code>, <code class="language-plaintext highlighter-rouge">Mistral-7B-Instruct-v0.3</code>,and <code class="language-plaintext highlighter-rouge">OLMoE-1B-7B-0924-Instruct</code> (on all benchmarks) to obtain the following overall variance decompositions:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">prompt perturbation</th>
      <th style="text-align: right"><code class="language-plaintext highlighter-rouge">Meta-Llama-3-8B-Instruct</code></th>
      <th style="text-align: right"><code class="language-plaintext highlighter-rouge">Mistral-7B-Instruct-v0.3</code></th>
      <th style="text-align: right"><code class="language-plaintext highlighter-rouge">OLMoE-1B-7B-0924-Instruct</code></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">total</td>
      <td style="text-align: right">2368.475849</td>
      <td style="text-align: right">1377.250181</td>
      <td style="text-align: right">2082.808656</td>
    </tr>
    <tr>
      <td style="text-align: left">enumerator</td>
      <td style="text-align: right">279.759745</td>
      <td style="text-align: right">121.579405</td>
      <td style="text-align: right">130.161762</td>
    </tr>
    <tr>
      <td style="text-align: left">separator</td>
      <td style="text-align: right">3.722368</td>
      <td style="text-align: right">25.363689</td>
      <td style="text-align: right">20.784928</td>
    </tr>
    <tr>
      <td style="text-align: left">choices order</td>
      <td style="text-align: right">190.237480</td>
      <td style="text-align: right">85.748062</td>
      <td style="text-align: right">12.895170</td>
    </tr>
    <tr>
      <td style="text-align: left">phrasing</td>
      <td style="text-align: right">566.256863</td>
      <td style="text-align: right">319.026200</td>
      <td style="text-align: right">231.941715</td>
    </tr>
    <tr>
      <td style="text-align: left">residual</td>
      <td style="text-align: right">1328.499393</td>
      <td style="text-align: right">825.532825</td>
      <td style="text-align: right">1687.025081</td>
    </tr>
  </tbody>
</table>

<p>We see that</p>

<ol>
  <li>the largest fraction of the variance in the LLMs’ accuracies is due to rephrasing the prompt, but changing the enumerator and answer choices order also account for significant fractions of the variance</li>
  <li>changing the separator seems to have almost no effect on the LLMs’ accuracies (even in aggregate);</li>
  <li>there is a significant fraction of residual variance that cannot be attributed to the main effects; we must consider interactions among prompt perturbations to explain this residual variance.</li>
</ol>

<p>Finally, here is our consistency leaderboard that ranks LLMs by the (total) variance in their accuracies. Keep in mind that we are ranking models by their consistency: if a model is consistently bad (eg <code class="language-plaintext highlighter-rouge">Llama-3.2-1B-Instruct</code>), then it’ll rank highly on the consistency leaderboard. Right now, there are only 5 models on the leaderboard because there are only 5 models in DOVE Lite. The main bottleneck in the way of including more models on the leaderboard is performing more experiments to measure the accuracies of more LLMs under (more) prompt perturbations; the computational cost of evaluating (DoV) and (DoV2) is negligible on modern hardware.</p>

<script src="/assets/js/sorttable.js"></script>

<table class="sortable">
  <caption>
    <p><strong>LLM consistency leaderboard:</strong> click on the column headers to sort the rows by the entries in the respective columns</p>
  </caption>
  <thead>
    <tr>
      <th style="text-align: left;">LLM</th>
      <th style="text-align: right;">total</th>
      <th style="text-align: right;">enumerator</th>
      <th style="text-align: right;">separator</th>
      <th style="text-align: right;">choices order</th>
      <th style="text-align: right;">phrasing</th>
      <!-- <th style="text-align: right;">residual</th> -->
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left;"><code>Mistral-7B-Instruct-v0.3</code></td>
      <td style="text-align: right;">1377.250181</td>
      <td style="text-align: right;">121.579405</td>
      <td style="text-align: right;">25.363689</td>
      <td style="text-align: right;">85.748062</td>
      <td style="text-align: right;">319.026200</td>
      <!-- <td style="text-align: right;">825.532825</td> -->
    </tr>
    <tr>
      <td style="text-align: left;"><code>Llama-3.2-1B-Instruct</code></td>
      <td style="text-align: right;">1713.863161</td>
      <td style="text-align: right;">167.214951</td>
      <td style="text-align: right;">27.546341</td>
      <td style="text-align: right;">281.520735</td>
      <td style="text-align: right;">6.358699</td>
      <!-- <td style="text-align: right;">1231.222435</td> -->
    </tr>
    <tr>
      <td style="text-align: left;"><code>OLMoE-1B-7B-0924-Instruct</code></td>
      <td style="text-align: right;">2082.808656</td>
      <td style="text-align: right;">130.161762</td>
      <td style="text-align: right;">20.784928</td>
      <td style="text-align: right;">12.895170</td>
      <td style="text-align: right;">231.941715</td>
      <!-- <td style="text-align: right;">1687.025081</td> -->
    </tr>
    <tr>
      <td style="text-align: left;"><code>Llama-3.2-3B-Instruct</code></td>
      <td style="text-align: right;">2305.981318</td>
      <td style="text-align: right;">470.136490</td>
      <td style="text-align: right;">18.672984</td>
      <td style="text-align: right;">205.075207</td>
      <td style="text-align: right;">208.307431</td>
      <!-- <td style="text-align: right;">1403.789206</td> -->
    </tr>
    <tr>
      <td style="text-align: left;"><code>Meta-Llama-3-8B-Instruct</code></td>
      <td style="text-align: right;">2368.475849</td>
      <td style="text-align: right;">279.759745</td>
      <td style="text-align: right;">3.722368</td>
      <td style="text-align: right;">190.237480</td>
      <td style="text-align: right;">566.256863</td>
      <!-- <td style="text-align: right;">1328.499393</td> -->
    </tr>
  </tbody>
</table>

<blockquote>
  <p>A notebook to replicate (the numbers on) the leaderboard is available at <i class="fab fa-github fa-fw"></i> / <a href="https://github.com/sundai-research">sundai-research</a> / <a href="https://github.com/sundai-research/consistency-leaderboard">consistency-leaderboard</a>.</p>
</blockquote>


<p><small>Hacked together by Seamus Somerstep and Yuekai Sun on March 23, 2025
</small></p>

      </section>

      <footer>
  <p><small>Hosted on <a href="http://github.com">GitHub</a> | Powered by <a href="http://jekyllrb.com/">Jekyll</a></small></p>
</footer>

    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
    <script type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
    
    
  </body>
</html>